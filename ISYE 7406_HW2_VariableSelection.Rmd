---
title: "ISYE 7406 - HW2"
output: html_notebook

--
****TOPIC: VARIABLE SELECTION***

Load Libraries.
```{r}
library(leaps)
```

Read in Data.
```{r}

#read in data
data0 <- read.csv("C:/Users/Britt/OneDrive/School/Georgia Tech Masters/4_Spring_2022/ISYE 7406_Data Mining and Statistical Learning/Homework/HW2_DataFile_Fat.csv", header= TRUE, sep = ",")

#summary
head(data0) #252 rows, 18 vars
summary(data0)

```

Split Data: Training & Testing
(90/10, per HW instructions so we can use explicitly observations)
```{r}
#randomly select testing data.
set.seed(7406) #seed specified in HW
n = dim(data0)[1] #total observations
n1 = round(n/10) #num randomly selected for test dataset

flag = sort(sample(1:n, n1))

#split data.
dataTrain = data0[-flag,] #exclude randomly selected obs 
dataTest = data0[flag,] #only include randomly selected obs 

#view training and testing data.
dim(dataTrain) #training data: 227 x 18
residuals(dataTest) #testing data: 25 x 18
```
Exploratory Analysis on Training Data.



```

Create Variables to Store Training & Testing Errors
```{r}
MSETrain <- NULL
MSETest <- NULL
yTrue <- dataTest$brozek
```


Training Data
Linear Regression Model - All Variables
```{r}
#linear regression model 
lm1 <- lm(brozek ~., data = dataTrain)
summary(lm1)
plot(lm1)

#training error
MSEMod1Train <-   mean((resid(lm1))^2);
MSETrain <- c(MSETrain, MSEMod1Train);

#testing error
pred0 <- predict(lm1, dataTest[,1:18]);
MSEMod1Test <- mean((pred0 - yTrue)^2);
MSETest <- c(MSETest, MSEMod1Test); 

#print training and testing error
MSEMod1Train
MSEMod1Test
```

**Training Data**

Linear Regression Variable Selection - Best Subset of k = 5 
*Model selection via exhaustive searchL forward, backward, etc.*
```{r}
#Create subset model 
sublm <- regsubsets(brozek ~., data = dataTrain, nbest = 100, really.big = TRUE)

subModel <- summary(sublm)$which
subModelSize <- as.numeric(attr(subModel, "dimnames")[[1]])
subModelRSS <- summary(sublm)$rss
#plot()

#view
head(subModel)
head(subModelSize)
head(subModelRSS)

#plot error vs k size (e.g., k = 1, 2, 3, etc.)
plot(subModelSize, subModelRSS)

#smallest RSS by size (ordered)
subModelBestRSS <- tapply(subModelRSS, subModelSize, min)
print(subModelBestRSS)

#RSS values for various k values
plot(1:8, subModelBestRSS, type = "b", col= "blue", xlab="Subset Size k", ylab="Residual Sum-of-Square")
points(subModelSize, subModelRSS)

#best subset when k = 5 (per HW instructions)
kFive <- which(subModelSize == 5)
flagFive <- kFive[which.min(subModelRSS[kFive])]

#fit best subset mode when k = 5
subModel[flagFive,]
bestMod <- subModel[flagFive,]
bestModName <- paste(names(bestMod)[bestMod][-1], collapse = "+")
bestForm <- paste("brozek ~", bestModName)

mod2 <- lm(as.formula(bestForm), data = dataTrain)
#model training error
MSEMod2Train <- mean(resid(mod2)^2)
MSETrain <- c(MSETrain, MSEMod2Train)

#model testing error
pred1 <- predict(mod2, dataTest[,1:18])
MSEMod2Test <- mean((pred1 - yTrue)^2)
MSETest <- c(MSETest, MSEMod2Test)

#training and testing errors
MSETrain
MSETest

```

Linear Regression Variable Selection - Stepwise AIC.
*Minimize AIC criterion*
```{r}
steplm <- lm(brozek ~ ., data = dataTrain)

```









